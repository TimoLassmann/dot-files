#+TITLE:  Setting up a Nimbus slurm cluster
#+AUTHOR: Timo Lassmann
#+EMAIL:  timo.lassmann@telethonkids.org.au
#+DATE:   2018-05-24
#+LATEX_CLASS: report
#+OPTIONS:  toc:nil
#+OPTIONS: H:4
#+LATEX_CMD: xelatex

* Introduction

  This tutorial walks you through the steps required to set up a linux cluster on
  pawsey's openstack nimbus cloud.


* Setting up and starting nodes 

  We highly recommend allocating a minimal amount of storage to each node and use
  a single large NFS mounted volume to share data within the cluster. 

1) Follow the instructions here:
  https://support.pawsey.org.au/documentation/display/US/Nimbus+-+Launching+an+Instance

  NOTE: some of the downstream steps will be easier if you create 5 nodes at once.
  This can be accomplished by selecting a name like 'node' and requesting 5 or
  more instances. OpenStack will append '-1', '-2' etc to the names. 

2) What can we do without emacs? 

#+BEGIN_SRC shell 
sudo apt update -yq 
sudo apt install emacs pdsh git autoconf automake clang llvm libtool cmake 
#+END_SRC


3) copy ssh key to main node connected to the internet
(so we can connect to individual nodes..)

4) On the main node add a ssh config file 

  Here is an example of how this may look: 

  #+BEGIN_SRC shell :tangle ssh_config_nimbus  :exports code :results none
    StrictHostKeyChecking no
    Host node-1 
    User ubuntu
    Hostname 192.168.1.166
    IdentityFile ~/.ssh/pawsey.key
    Host node-2
    Hostname 192.168.1.29
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-3
    Hostname 192.168.1.10
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-4
    Hostname 192.168.1.31
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-5
    Hostname 192.168.1.99
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-6
    Hostname 192.168.1.67
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
  #+END_SRC

  or: 

  #+BEGIN_SRC shell :tangle ssh_config_nimbus  :exports code :results none
    StrictHostKeyChecking no
    Host rare-1 
    User ubuntu
    Hostname 192.168.1.6
    IdentityFile ~/.ssh/pawsey.key
    Host rare-2
    Hostname 192.168.1.22
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host rare-3
    Hostname 192.168.1.27
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host rare-4
    Hostname 192.168.1.20
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host rare-5
    Hostname 192.168.1.36
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host rare-6
    Hostname 192.168.1.16
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
  #+END_SRC


  Then copy to content to ~/.ssh/config 


  5) Install pdsh as describes here

  https://support.pawsey.org.au/documentatnion/display/US/Nimbus+-+Managing+a+VM+Cluster

#+BEGIN_SRC shell 
sudo apt-get install pdsh

#+END_SRC
  Especially add nodes in the /etc/genders file:  


  sudo vi /etc/genders

  #+BEGIN_SRC shell :tangle genders  :exports code :results none
    node-2
    node-3
    node-4
    node-5
    node-6
  #+END_SRC

File =/etc/pdsh/rcmd_default= needs to contain =ssh=. 

To test: 
#+BEGIN_SRC shell 
pdsh -a 'ip a | grep 192.168'
#+END_SRC

* Update nodes

  #+BEGIN_SRC shell
    pdsh -a sudo apt-get update 

  #+END_SRC

* Setup MUNGE & SLURM

** Create users

#+BEGIN_SRC bash :noweb yes :tangle create_munge_slurm_users.sh :shebang #!/usr/bin/env bash

# ----------------------------------------------------------------------
#  create_munge_slurm_users.sh (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: 
# ----------------------------------------------------------------------
sudo userdel munge
sudo groupdel munge

sudo userdel slurm
sudo groupdel slurm 


export MUNGEUSER=966
sudo groupadd -g $MUNGEUSER munge
sudo useradd  -m -c "MUNGE Uid 'N' Gid Emporium" -d /var/lib/munge -u $MUNGEUSER -g munge  -s /sbin/nologin munge
export SLURMUSER=967
sudo groupadd -g $SLURMUSER slurm
sudo useradd  -m -c "SLURM workload manager" -d /var/lib/slurm -u $SLURMUSER -g slurm  -s /bin/bash slurm

# ----------------------------------------------------------------------
#+END_SRC

#+BEGIN_SRC bash
./create_munge_slurm_users.sh

pdcp -a create_munge_slurm_users.sh .
pdsh -a ./create_munge_slurm_users.sh
#+END_SRC



** requirements :
#+BEGIN_SRC shell
sudo apt install mariadb-server libmariadbclient-dev libmariadb-dev -y
sudo apt install python3 gcc openssl numactl hwloc lua5.3 man2html make ruby ruby-dev libmunge-dev libpam0g-dev -y
sudo /usr/bin/gem install fpm

pdsh -a sudo apt install mariadb-server libmariadbclient-dev libmariadb-dev -y
pdsh -a sudo apt install python3 gcc openssl numactl hwloc lua5.3 man2html make ruby ruby-dev libmunge-dev libpam0g-dev -y
pdsh -a sudo /usr/bin/gem install fpm
 #+END_SRC




** MUNGE

*** Install 
    #+BEGIN_SRC shell :tangle basic_node_setup.sh :shebang #!/bin/bash :exports code :results none

      pdsh -a sudo apt-get -yq install libmunge-dev libmunge2 munge pdsh
      sudo apt-get -yq install libmunge-dev libmunge2 munge pdsh
      
    #+END_SRC

*** Synchronise munge keys.

    First set up a script to copy the munge key from the home directory into the
    right location on all node.
    #+BEGIN_SRC shell :tangle munge_per_node.sh :shebang #!/bin/bash :exports code :results none
      sudo systemctl stop munge
      cd ~
      sudo chown munge: munge.key
      sudo cp munge.key /etc/munge/munge.key

      sudo chmod 400 /etc/munge/munge.key
      sudo systemctl enable munge
      sudo systemctl start munge
    #+END_SRC

    Then: 
    1) generate the munge key on the head node
    2) copy the key to all nodes 
    3) run the script above 
    4) also copy the key to the correct directory on the head node 

       #+BEGIN_SRC shell :tangle generate_sync_munge_key.sh :shebang #!/bin/bash :exports code :results none
         dd if=/dev/urandom bs=1 count=1024 > munge.key

         pdcp -a munge.key ~/munge.key
         pdcp -a munge_per_node.sh  ~/munge_per_node.sh 
         pdsh -a ./munge_per_node.sh 

         sudo systemctl stop munge
         sudo chown munge: munge.key
         sudo cp munge.key /etc/munge/munge.key

         sudo chmod 400 /etc/munge/munge.key

         sudo systemctl enable munge
         sudo systemctl start munge

       #+END_SRC

Testing!!! 
#+BEGIN_SRC shell 
munge -n | unmunge
munge -n | ssh node-2 unmunge
munge -n | ssh node-3 unmunge
munge -n | ssh node-4 unmunge
munge -n | ssh node-5 unmunge
munge -n | ssh node-6 unmunge
remunge
#+END_SRC


* Slurm from source:



#+BEGIN_SRC bash :noweb yes :tangle makeslurm_from_source.sh :shebang #!/usr/bin/env bash

# ----------------------------------------------------------------------
#  makeslurm_from_source.sh (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: copied from: https://www.ni-sp.com/slurm-build-script-and-container-commercial-support/#h-automatic-slurm-build-script-for-ubuntu-18-04-and-20-04
# ----------------------------------------------------------------------
mkdir -p slurm-tmp
cd slurm-tmp
if [ "$VER" == "" ]; then
    export VER=20.02.6    # latest 20.02.XX version
    export VER=20.11.5
fi
# https://download.schedmd.com/slurm/slurm-20.02.3.tar.bz2
wget https://download.schedmd.com/slurm/slurm-$VER.tar.bz2

tar jxvf slurm-$VER.tar.bz2
cd slurm-$VER
# ./configure
./configure --prefix=/usr --sysconfdir=/etc/slurm --enable-pam --with-pam_dir=/lib/x86_64-linux-gnu/security/ --without-shared-libslurm
make
make contrib
sudo make install
cd ..
# fpm -s dir -t deb -v 1.0 -n slurm-$VER --prefix=/usr -C /tmp/slurm-build .
# echo Creating deb package for SLURM $VER
# fpm -s dir -t deb -v 1.0 -n slurm-$VER --prefix=/usr -C /usr .
# ----------------------------------------------------------------------
#+END_SRC

#+RESULTS:
Run and install: 
#+BEGIN_SRC bash
pdcp -a makeslurm_from_source.sh . 
pdsh -a ./makeslurm_from_source.sh  
pdsh -a slurmd --version
slurmd --version

sudo mkdir -p /etc/slurm /etc/slurm/prolog.d /etc/slurm/epilog.d /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm
sudo chown slurm /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm

pdsh -a sudo mkdir -p /etc/slurm /etc/slurm/prolog.d /etc/slurm/epilog.d /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm
pdsh -a sudo chown slurm /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm
#+END_SRC

Create all auxillary directories:

#+BEGIN_SRC bash :noweb yes :tangle create_slurm_directories.sh :shebang #!/usr/bin/env bash

# ----------------------------------------------------------------------
#  create_slurm_directories.sh (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: Creates / touches all files needed by the slurm daemon
# ----------------------------------------------------------------------

sudo mkdir -p /var/spool/slurm
sudo chown slurm:slurm /var/spool/slurm
sudo chmod 755 /var/spool/slurm
sudo mkdir -p /var/spool/slurm/slurmctld
sudo chown slurm:slurm /var/spool/slurm/slurmctld
sudo chmod 755 /var/spool/slurm/slurmctld
sudo mkdir -p /var/spool/slurm/cluster_state
sudo chown slurm:slurm /var/spool/slurm/cluster_state
sudo touch /var/log/slurmctld.log
sudo chown slurm:slurm /var/log/slurmctld.log
sudo touch /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
sudo chown slurm: /var/log/slurm_jobacct.log /var/log/slurm_jobcomp.log
sudo touch /var/run/slurmctld.pid /var/run/slurmd.pid
sudo chown slurm:slurm /var/run/slurmctld.pid /var/run/slurmd.pid
sudo mkdir -p /etc/slurm/prolog.d /etc/slurm/epilog.d 

# ----------------------------------------------------------------------
#+END_SRC


#+BEGIN_SRC bash

./create_slurm_directories.sh
pdcp -a create_slurm_directories.sh .
pdsh -a ./create_slurm_directories.sh
#+END_SRC

#+BEGIN_SRC bash :noweb yes :tangle slurm.conf

# slurm.conf file generated by configurator.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=node-1
MpiDefault=none
ProctrackType=proctrack/cgroup
ReturnToService=2
#SallocDefaultCommand=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm/slurmd
SlurmUser=slurm
#SlurmdUser=slurm
#SrunEpilog=
#SrunProlog=
StateSaveLocation=/var/spool/slurm/
SwitchType=switch/none
#TaskEpilog=
TaskPlugin=task/affinity
#
# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_res
SelectTypeParameters=CR_Core_Memory

# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
#AccountingStorageHost=
#AccountingStorageLoc=
#AccountingStoragePass=
#AccountingStoragePort=
AccountingStorageType=accounting_storage/none
#AccountingStorageUser=
#AccountingStoreJobComment=YES
ClusterName=cluster
#DebugFlags=
#JobCompHost=
#JobCompLoc=
#JobCompPass=
#JobCompPort=
JobCompType=jobcomp/none
#JobCompUser=
#JobContainerType=job_container/none
#JobAcctGatherFrequency=30
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=info
SlurmctldLogFile=/var/log/slurmctld.log
SlurmdDebug=info
SlurmdLogFile=/var/log/slurmd.log
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES (optional)
#SuspendProgram=
#ResumeProgram=
#SuspendTimeout=
#ResumeTimeout=
#ResumeRate=
#SuspendExcNodes=
#SuspendExcParts=
#SuspendRate=
#SuspendTime=
#
#
# COMPUTE NODES
NodeName=node-[2-6] CPUs=32 Boards=1 SocketsPerBoard=1 CoresPerSocket=16 ThreadsPerCore=2 RealMemory=128831
PartitionName=debug Nodes=node-[2-6] Default=YES MaxTime=INFINITE State=UP

#+END_SRC

Copy slurm config file into correct location 
#+BEGIN_SRC bash
sudo cp slurm.conf  /etc/slurm/slurm.conf
pdcp -a slurm.conf .
pdsh -a sudo cp slurm.conf  /etc/slurm/slurm.conf
#+END_SRC


** cgroups

#+BEGIN_SRC bash :noweb yes :tangle cgroup.conf

# ----------------------------------------------------------------------
#  cgroup.conf (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: cgroup file 
# ----------------------------------------------------------------------

###
#
# Slurm cgroup support configuration file
#
# See man slurm.conf and man cgroup.conf for further
# information on cgroup configuration parameters
#--
CgroupAutomount=yes

ConstrainCores=no
ConstrainRAMSpace=no

# ----------------------------------------------------------------------
#+END_SRC

#+BEGIN_SRC bash
     sudo cp cgroup.conf /etc/slurm/cgroup.conf
     pdcp -a  cgroup.conf  .
     pdsh -a sudo cp cgroup.conf /etc/slurm/cgroup.conf
#+END_SRC


** service files

#+BEGIN_SRC bash :noweb yes :tangle slurmctld.service

# ----------------------------------------------------------------------
#  slurmctld.service (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: 
# ----------------------------------------------------------------------

[Unit]
Description=Slurm controller daemon
After=network.target munge.service
ConditionPathExists=/etc/slurm/slurm.conf

[Service]
Type=forking
EnvironmentFile=-/etc/sysconfig/slurmctld
ExecStart=/usr/sbin/slurmctld $SLURMCTLD_OPTIONS
ExecReload=/bin/kill -HUP \$MAINPID
PIDFile=/var/run/slurmctld.pid

[Install]
WantedBy=multi-user.target

# ----------------------------------------------------------------------
#+END_SRC

#+BEGIN_SRC bash :noweb yes :tangle slurmdbd.service

# ----------------------------------------------------------------------
#  slurmdbd.service (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: 
# ----------------------------------------------------------------------

[Unit]
Description=Slurm DBD accounting daemon
After=network.target munge.service
ConditionPathExists=/etc/slurm/slurmdbd.conf

[Service]
Type=forking
EnvironmentFile=-/etc/sysconfig/slurmdbd
ExecStart=/usr/sbin/slurmdbd $SLURMDBD_OPTIONS
ExecReload=/bin/kill -HUP \$MAINPID
PIDFile=/var/run/slurmdbd.pid

[Install]
WantedBy=multi-user.target


# ----------------------------------------------------------------------
#+END_SRC

#+BEGIN_SRC bash
sudo cp slurmctld.service /etc/systemd/system/slurmctld.service
sudo cp slurmdbd.service  /etc/systemd/system/slurmdbd.service
#+END_SRC



** Start master node

#+BEGIN_SRC bash
sudo systemctl daemon-reload
sudo systemctl enable slurmdbd
sudo systemctl start slurmdbd
sudo systemctl enable slurmctld
sudo systemctl start slurmctld


#+END_SRC

** Compute nodes:

#+BEGIN_SRC bash :noweb yes :tangle slurmd.service 

# ----------------------------------------------------------------------
#  slurmd.service  (tangled from: SETUP_NIMBUS_MINIMAL.ORG)
#  Descriptions: 
# ----------------------------------------------------------------------
[Unit]
Description=Slurm node daemon
After=network.target munge.service
ConditionPathExists=/etc/slurm/slurm.conf

[Service]
Type=forking
EnvironmentFile=-/etc/sysconfig/slurmd
ExecStart=/usr/sbin/slurmd -d /usr/sbin/slurmstepd $SLURMD_OPTIONS
ExecReload=/bin/kill -HUP \$MAINPID
PIDFile=/var/run/slurmd.pid
KillMode=process
LimitNOFILE=51200
LimitMEMLOCK=infinity
LimitSTACK=infinity

[Install]
WantedBy=multi-user.target

# ----------------------------------------------------------------------
#+END_SRC

#+BEGIN_SRC bash 

sudo cp slurmd.service  /etc/systemd/system/slurmd.service
pdcp -a  slurmd.service .
pdsh -a sudo  cp slurmd.service  /etc/systemd/system/slurmd.service
pdsh -a sudo systemctl daemon-reload
pdsh -a sudo systemctl enable slurmd.service
pdsh -a sudo systemctl start slurmd.service
#+END_SRC



# on compute nodes 


# ----------------------------------------------------------------------
#+END_SRC


* Slurm 

  Setup for a basic slurm scheduler. The setup below is minimal but works.  
  Install slurm

  Let's try to install the deb slurm package from above... 
  
  
#+BEGIN_SRC bash 
  pdsh -a sudo apt install slurmctld slurmd -yq
#+END_SRC
** Generate a slurm config file 

   Generate a slurm config file using this web-form: 

   https://slurm.schedmd.com/configurator.html

   Note: 
   1) It is critical to correctly name the head node and the worker nodes. These
      names have to match exactly what is shown in the openstack online management
      console (https://nimbus.pawsey.org.au).

   2) Make sure the number of CPUs, main memory etc are set correctly for each
      worker node. If you are unsure how to set these install the slurm daemon on a
      node: 
      ssh nodeXXX 
      sudo apt-get install -yq slurmd 
      and run: 
      slurmd -C 
   3) Make sure the =SlurmctldPidFile= and =SlurmdPidFile= variables
      point to directories writable bu the slurm user (on ubuntu 18
      this is =/var/run/slurm-llnl=)

   Finally copy the output of the configurator webpage into a file called
   =slurm.conf= in the home directory of your head node. 


** Script to set up node 

   The script below is used to configure each node. This includes moving the
   slurm.conf file created above into the right directory. Note that the IP
   addresses have to be manually edited in this script to match your setup.

   #+BEGIN_SRC shell :tangle setup_host_for_slurm.sh :shebang #!/bin/bash   :exports code :results none
     sudo -- sh -c 'cat > /etc/slurm-llnl/cgroup.conf  << "EOF"
     CgroupAutomount=yes
     CgroupReleaseAgentDir="/etc/slurm-llnl/cgroup" 

     ConstrainCores=yes 
     ConstrainDevices=yes
     ConstrainRAMSpace=yes


     EOF'

     sudo -- sh -c 'cat > /etc/hosts  << "EOF"
     127.0.0.1 localhost
     192.168.1.29 node-1
     192.168.1.18 node-2
     192.168.1.15 node-3
     192.168.1.9 node-4
     192.168.1.7 node-5
     192.168.1.5 node-6
     # The following lines are desirable for IPv6 capable hosts
     ::1 ip6-localhost ip6-loopback
     fe00::0 ip6-localnet
     ff00::0 ip6-mcastprefix
     ff02::1 ip6-allnodes
     ff02::2 ip6-allrouters
     ff02::3 ip6-allhosts

     EOF'

     sudo cp ~/slurm.conf /etc/slurm-llnl/slurm.conf
     sudo chown slurm: /etc/slurm-llnl/slurm.conf


   #+END_SRC

Set permission  (has to match spool entry in slurm.conf - otherwise
daemon can't write...)

#+BEGIN_SRC shell 
sudo mkdir -p /var/spool/slurmd
sudo mkdir -p /var/lib/slurm-llnl
sudo mkdir -p /var/lib/slurm-llnl/slurmd
sudo mkdir -p /var/lib/slurm-llnl/slurmctld
sudo mkdir -p /var/run/slurm-llnl
sudo mkdir -p /var/log/slurm-llnl


sudo chmod -R 755 /var/spool/slurmd
sudo chmod -R 755 /var/lib/slurm-llnl/
sudo chmod -R 755 /var/run/slurm-llnl/
sudo chmod -R 755 /var/log/slurm-llnl/

sudo chown -R slurm:slurm /var/spool/slurmd
sudo chown -R slurm:slurm /var/lib/slurm-llnl/
sudo chown -R slurm:slurm /var/run/slurm-llnl/
sudo chown -R slurm:slurm /var/log/slurm-llnl/

pdsh -a sudo mkdir -p /var/spool/slurmd
pdsh -a sudo mkdir -p /var/lib/slurm-llnl
pdsh -a sudo mkdir -p /var/lib/slurm-llnl/slurmd
pdsh -a sudo mkdir -p /var/lib/slurm-llnl/slurmctld
pdsh -a sudo mkdir -p /var/run/slurm-llnl
pdsh -a sudo mkdir -p /var/log/slurm-llnl


pdsh -a sudo chmod -R 755 /var/spool/slurmd
pdsh -a sudo chmod -R 755 /var/lib/slurm-llnl/
pdsh -a sudo chmod -R 755 /var/run/slurm-llnl/
pdsh -a sudo chmod -R 755 /var/log/slurm-llnl/

pdsh -a sudo chown -R slurm:slurm /var/spool/slurmd
pdsh -a sudo chown -R slurm:slurm /var/lib/slurm-llnl/
pdsh -a sudo chown -R slurm:slurm /var/run/slurm-llnl/
pdsh -a sudo chown -R slurm:slurm /var/log/slurm-llnl/



#+END_SRC

   The following script configures the head node, copies the above script to all
   nodes and configures them.

   NOTE:

   1) Edit the IP address to match your configuration.
   2) this script expects a =slurm.conf= file in the home directory of user ubuntu
   on the head node.

   #+BEGIN_SRC shell :tangle setup_hosts_file.sh :shebang #!/bin/bash :exports code :results none
     sudo apt-get  install slurmctld slurmdbd -yq 
     pdsh -a sudo apt-get update  
     pdsh -a sudo apt-get -yq install slurmd pdsh
     pdcp -a slurm.conf ~/slurm.conf
     pdcp -a setup_host_for_slurm.sh  ~/setup_host_for_slurm.sh
     pdsh -a ./setup_host_for_slurm.sh 
     pdsh -a grep "node-1" /etc/hosts 

     sudo cp ~/slurm.conf /etc/slurm-llnl/slurm.conf
     sudo chown slurm: /etc/slurm-llnl/slurm.conf
     sudo -- sh -c 'cat > /etc/hosts  << "EOF"
     127.0.0.1 localhost
     192.168.1.29 node-1
     192.168.1.18 node-2
     192.168.1.15 node-3
     192.168.1.9 node-4
     192.168.1.7 node-5
     192.168.1.5 node-6

     # The following lines are desirable for IPv6 capable hosts
     ::1 ip6-localhost ip6-loopback
     fe00::0 ip6-localnet
     ff00::0 ip6-mcastprefix
     ff02::1 ip6-allnodes
     ff02::2 ip6-allrouters
     ff02::3 ip6-allhosts

     EOF'


   #+END_SRC

** Start Slurm 

First let's start the work nodes 

#+BEGIN_SRC shell :exports code :results none
  pdsh -a sudo systemctl enable slurmd.service
  pdsh -a sudo systemctl stop slurmd.service

  pdsh -a sudo systemctl start slurmd.service

#+END_SRC


Now let's see if we can start the server node: 


#+BEGIN_SRC shell :exports code :results none
  sudo systemctl enable slurmctld.service
  sudo systemctl stop slurmctld.service
  sudo systemctl start slurmctld.service
  sudo systemctl status slurmctld.service

#+END_SRC

Test if everything is ok: 

#+BEGIN_SRC shell :exports code :results none



scontrol show nodes
#+END_SRC


   After this you should be able to see all nodes using the =sinfo= command. 

Troubleshooting: 

Sometimes the nodes appear to have low memory. To fix this: 

#+BEGIN_SRC shell 
  sudo scontrol 

  update NodeName=node-2 State=DOWN Reason="undraining"

  update NodeName=node-2 State=RESUME

  update NodeName=node-3 State=DOWN Reason="undraining"
  update NodeName=node-3 State=RESUME

  update NodeName=node-4 State=DOWN Reason="undraining"
  update NodeName=node-4 State=RESUME

  update NodeName=node-5 State=DOWN Reason="undraining"
  update NodeName=node-5 State=RESUME

  update NodeName=node-6 State=DOWN Reason="undraining"
  update NodeName=node-6 State=RESUME

#+END_SRC


* Sharing a mounted volume via NFS  

  The following script starts a nfs server on the head node and runs code to mount
  the volume on all work nodes. 

  The IP addresses in the =/etc/exports= file belong to the nodes; the IP address
  in the final mount command is the head node's.

  #+BEGIN_SRC shell :exports code :results none
    pdsh -a sudo apt-get -yq install nfs-common
    sudo apt-get install -yq nfs-kernel-server

    sudo -- sh -c 'cat > /etc/exports  << "EOF"
        /data 192.168.1.18(rw,sync,no_subtree_check)
        /data 192.168.1.15(rw,sync,no_subtree_check)
        /data 192.168.1.9(rw,sync,no_subtree_check)
        /data 192.168.1.7(rw,sync,no_subtree_check)
        /data 192.168.1.5(rw,sync,no_subtree_check)
    EOF'

    sudo /etc/init.d/rpcbind restart
    sudo /etc/init.d/nfs-kernel-server restart
    sudo exportfs -r

    pdsh -a sudo mkdir /data
    pdsh -a sudo chown -R ubuntu:ubuntu /data 
    pdsh -a sudo mount 192.168.1.29:/data /data      

    pdsh -a ls /data 

  #+END_SRC

  To stop the NFS server (i.e. before un-mounting the volume): 

  service nfs-kernel-server stop



* Install Docker 

  #+BEGIN_SRC shell :exports code :results none
    pdsh -a sudo apt  install docker.io -yq 
  #+END_SRC

  Add myself to docker group... 
  #+BEGIN_SRC shell
pdsh -a sudo gpasswd -a $USER docker
  #+END_SRC

* Install R (3.5+) on all nodes 

#+BEGIN_SRC shell :exports code :results none 

  sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
  sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'
  pdsh -a sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
  pdsh -a "sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'"
  pdsh -a sudo apt update 

  sudo apt install r-base -yq 
  pdsh -a sudo apt install r-base -yq


  sudo apt install build-essential -yq
  pdsh -a sudo apt install build-essential -yq

  pdsh -a sudo apt install -yq default-jdk libxml2-dev libcurl4-openssl-dev openssl libssl-dev




  libssh2-1-dev libxml2-dev libcurl4-openssl-dev libssl-dev  
  pdsh -a "sudo apt-get install -yq libhdf5-dev"
#+END_SRC

* Install R packages 

  Need to add double quotes for pdsh to work - all other double quotes need to be
  escaped.

  #+BEGIN_SRC sh

    pdsh -a "sudo R -e 'install.packages(c(\"Seurat\",\"tidyverse\",\"optparse\",\"reshape2\",\"devtools\",\"reshape\"), repos=\"http://cran.us.r-project.org\", Ncpus = 16)'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"GSVA\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"biomaRt\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"DropletUtils\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"ensembldb\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"EnsDb.Hsapiens.v86\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"EnsDb.Mmusculus.v79\")'"
    pdsh -a "sudo R -e 'devtools::install_github(\"dviraran/SingleR\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"scater\")'"
    pdsh -a "sudo R -e 'source(\"http://cf.10xgenomics.com/supp/cell-exp/rkit-install-2.0.0.R\")'"

  #+END_SRC

  #+RESULTS:

 #+Begin_SRC sh
   sudo apt-get install -yq libhdf5-dev
   sudo R -e 'install.packages(c("Seurat","tidyverse","optparse","reshape2","devtools","reshape"),dependencies = TRUE, repos="http://cran.us.r-project.org", Ncpus = 16)'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("GSVA")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("biomaRt")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("DropletUtils")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("ensembldb")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("EnsDb.Hsapiens.v86")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("EnsDb.Mmusculus.v79")'
   sudo R -e 'devtools::install_github("dviraran/SingleR")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("scater")'
   sudo R -e 'source("http://cf.10xgenomics.com/supp/cell-exp/rkit-install-2.0.0.R")'

  #+END_SRC

  #+RESULTS:

* Add users


  #+BEGIN_SRC sh 

    sudo addgroup --gid 1005 analysis 
    sudo useradd -m -b  /home -d /home/melvin  -g 1005 -u 1001 melvin
    sudo useradd -m -b  /home -d /home/andre  -g 1005 -u 1002 andre
    pdsh -a sudo addgroup --gid 1005 analysis 
    pdsh -a sudo useradd -m -b  /home -d /home/melvin  -g 1005 -u 1001 melvin
    pdsh -a sudo useradd -m -b  /home -d /home/andre  -g 1005 -u 1002 andre


  #+END_SRC

  Set password

  #+BEGIN_SRC sh 
    sudo passwd andre
    sudo passwd melvin 
  #+END_SRC
 
