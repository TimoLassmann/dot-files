#+TITLE:  Setting up a Nimbus slurm cluster
#+AUTHOR: Timo Lassmann
#+EMAIL:  timo.lassmann@telethonkids.org.au
#+DATE:   2018-05-24
#+LATEX_CLASS: report
#+OPTIONS:  toc:nil
#+OPTIONS: H:4
#+LATEX_CMD: xelatex

* Introduction

  This tutorial walks you through the steps required to set up a linux cluster on
  pawsey's openstack nimbus cloud.


* Setting up and starting nodes 

  We highly recommend allocating a minimal amount of storage to each node and use
  a single large NFS mounted volume to share data within the cluster. 

1) Follow the instructions here:
  https://support.pawsey.org.au/documentation/display/US/Nimbus+-+Launching+an+Instance

  NOTE: some of the downstream steps will be easier if you create 5 nodes at once.
  This can be accomplished by selecting a name like 'node' and requesting 5 or
  more instances. OpenStack will append '-1', '-2' etc to the names. 

2) What can we do without emacs? 

#+BEGIN_SRC shell 
sudo apt update -yq 
sudo apt install emacs25 pdsh git autoconf automake clang llvm libtool cmake 
#+END_SRC


3) copy ssh key to main node connected to the internet
(so we can connect to individual nodes..)

4) On the main node add a ssh config file 

  Here is an example of how this may look: 

  #+BEGIN_SRC shell :tangle ssh_config_nimbus  :exports code :results none
    StrictHostKeyChecking no
    Host node-1 
    User ubuntu
    Hostname 192.168.1.12
    IdentityFile ~/.ssh/pawsey.key
    Host node-2
    Hostname 192.168.1.7
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-3
    Hostname 192.168.1.5
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-4
    Hostname 192.168.1.14
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-5
    Hostname 192.168.1.10
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
    Host node-6
    Hostname 192.168.1.9
    User ubuntu
    IdentityFile ~/.ssh/pawsey.key
  #+END_SRC

  Then copy to content to ~/.ssh/config 


  5) Install pdsh as describes here

  https://support.pawsey.org.au/documentatnion/display/US/Nimbus+-+Managing+a+VM+Cluster

#+BEGIN_SRC shell 
sudo apt-get install pdsh

#+END_SRC
  Especially add nodes in the /etc/genders file:  

  sudo vi /etc/genders

  #+BEGIN_SRC shell :tangle genders  :exports code :results none
    node-2
    node-3
    node-4
    node-5
    node-6
  #+END_SRC

File =/etc/pdsh/rcmd_default= needs to contain =ssh=. 

To test: 
#+BEGIN_SRC shell 
pdsh -a 'ip a | grep 192.168'
#+END_SRC

* Update nodes

  #+BEGIN_SRC shell
    pdsh -a sudo apt-get update 

  #+END_SRC

* Setup MUNGE & SLURM 
** MUNGE

*** Install 
    #+BEGIN_SRC shell :tangle basic_node_setup.sh :shebang #!/bin/bash :exports code :results none

      pdsh -a sudo apt-get -yq install libmunge-dev libmunge2 munge pdsh
      sudo apt-get -yq install libmunge-dev libmunge2 munge pdsh

    #+END_SRC

*** Synchronise munge keys.

    First set up a script to copy the munge key from the home directory into the
    right location on all node.

    #+BEGIN_SRC shell :tangle munge_per_node.sh :shebang #!/bin/bash :exports code :results none
      sudo systemctl stop munge
      cd ~
      sudo chown munge: munge.key
      sudo cp munge.key /etc/munge/munge.key

      sudo chmod 400 /etc/munge/munge.key
      sudo systemctl enable munge
      sudo systemctl start munge
    #+END_SRC

    Then: 
    1) generate the munge key on the head node
    2) copy the key to all nodes 
    3) run the script above 
    4) also copy the key to the correct directory on the head node 

       #+BEGIN_SRC shell :tangle generate_sync_munge_key.sh :shebang #!/bin/bash :exports code :results none
         dd if=/dev/urandom bs=1 count=1024 > munge.key

         pdcp -a munge.key ~/munge.key
         pdcp -a munge_per_node.sh  ~/munge_per_node.sh 
         pdsh -a ./munge_per_node.sh 

         sudo systemctl stop munge
         sudo chown munge: munge.key
         sudo cp munge.key /etc/munge/munge.key

         sudo chmod 400 /etc/munge/munge.key

         sudo systemctl enable munge
         sudo systemctl start munge

       #+END_SRC

Testing!!! 
#+BEGIN_SRC shell 
munge -n | unmunge
munge -n | ssh node-2 unmunge
munge -n | ssh node-3 unmunge
munge -n | ssh node-4 unmunge
munge -n | ssh node-5 unmunge
munge -n | ssh node-6 unmunge
remunge
#+END_SRC

* Slurm 

  Setup for a basic slurm scheduler. The setup below is minimal but works.  

** Generate a slurm config file 

   Generate a slurm config file using this web-form: 

   https://slurm.schedmd.com/configurator.html

   Note: 
   1) It is critical to correctly name the head node and the worker nodes. These
      names have to match exactly what is shown in the openstack online management
      console (https://nimbus.pawsey.org.au).

   2) Make sure the number of CPUs, main memory etc are set correctly for each
      worker node. If you are unsure how to set these install the slurm daemon on a
      node: 
      ssh nodeXXX 
      sudo apt-get install -yq slurmd 
      and run: 
      slurmd -C 
   3) Make sure the =SlurmctldPidFile= and =SlurmdPidFile= variables
      point to directories writable bu the slurm user (on ubuntu 18
      this is =/var/run/slurm-llnl=)

   Finally copy the output of the configurator webpage into a file called
   =slurm.conf= in the home directory of your head node. 


** Script to set up node 

   The script below is used to configure each node. This includes moving the
   slurm.conf file created above into the right directory. Note that the IP
   addresses have to be manually edited in this script to match your setup.

   #+BEGIN_SRC shell :tangle setup_host_for_slurm.sh :shebang #!/bin/bash   :exports code :results none
     sudo -- sh -c 'cat > /etc/slurm-llnl/cgroup.conf  << "EOF"
     CgroupAutomount=yes
     CgroupReleaseAgentDir="/etc/slurm-llnl/cgroup" 

     ConstrainCores=yes 
     ConstrainDevices=yes
     ConstrainRAMSpace=yes

     EOF'

     sudo -- sh -c 'cat > /etc/hosts  << "EOF"
     127.0.0.1 localhost
     192.168.1.12 node-1
     192.168.1.7 node-2
     192.168.1.5 node-3
     192.168.1.14 node-4
     192.168.1.10 node-5
     192.168.1.9 node-6
     # The following lines are desirable for IPv6 capable hosts
     ::1 ip6-localhost ip6-loopback
     fe00::0 ip6-localnet
     ff00::0 ip6-mcastprefix
     ff02::1 ip6-allnodes
     ff02::2 ip6-allrouters
     ff02::3 ip6-allhosts

     EOF'

     sudo cp ~/slurm.conf /etc/slurm-llnl/slurm.conf
     sudo chown slurm: /etc/slurm-llnl/slurm.conf


   #+END_SRC

Set permission  (has to match spool entry in slurm.conf - otherwise
daemon can't write...)

#+BEGIN_SRC shell 

mkdir /var/spool/slurmctld
chown slurm: /var/spool/slurmctld
chmod 755 /var/spool/slurmctld
#+END_SRC

   The following script configures the head node, copies the above script to all
   nodes and configures them.

   NOTE:

   1) Edit the IP address to match your configuration.
   2) this script expects a =slurm.conf= file in the home directory of user ubuntu
   on the head node.

   #+BEGIN_SRC shell :tangle setup_hosts_file.sh :shebang #!/bin/bash :exports code :results none
     sudo apt-get  install slurmctld slurmdbd -yq 
     pdsh -a sudo apt-get update  
     pdsh -a sudo apt-get -yq install slurmd pdsh
     pdcp -a slurm.conf ~/slurm.conf
     pdcp -a setup_host_for_slurm.sh  ~/setup_host_for_slurm.sh
     pdsh -a ./setup_host_for_slurm.sh 
     pdsh -a grep "node-1" /etc/hosts 

     sudo cp ~/slurm.conf /etc/slurm-llnl/slurm.conf
     sudo chown slurm: /etc/slurm-llnl/slurm.conf
     sudo -- sh -c 'cat > /etc/hosts  << "EOF"
     127.0.0.1 localhost
     192.168.1.12 node-1
     192.168.1.7 node-2
     192.168.1.5 node-3
     192.168.1.14 node-4
     192.168.1.10 node-5
     192.168.1.9 node-6

     # The following lines are desirable for IPv6 capable hosts
     ::1 ip6-localhost ip6-loopback
     fe00::0 ip6-localnet
     ff00::0 ip6-mcastprefix
     ff02::1 ip6-allnodes
     ff02::2 ip6-allrouters
     ff02::3 ip6-allhosts

     EOF'


   #+END_SRC

** Start Slurm 

First let's start the work nodes 

#+BEGIN_SRC shell :exports code :results none
  pdsh -a sudo systemctl enable slurmd.service
  pdsh -a sudo systemctl stop slurmd.service

  pdsh -a sudo systemctl start slurmd.service

#+END_SRC


Now let's see if we can start the server node: 


#+BEGIN_SRC shell :exports code :results none
  sudo systemctl enable slurmctld.service
  sudo systemctl stop slurmctld.service
  sudo systemctl start slurmctld.service
  sudo systemctl status slurmctld.service

#+END_SRC

Test if everything is ok: 

#+BEGIN_SRC shell :exports code :results none
scontrol show nodes
#+END_SRC


   After this you should be able to see all nodes using the =sinfo= command. 

Troubleshooting: 

Sometimes the nodes appear to have low memory. To fix this: 

#+BEGIN_SRC shell 
  sudo scontrol 

  update NodeName=node-2 State=DOWN Reason="undraining"
  update NodeName=node-2 State=RESUME

  update NodeName=node-3 State=DOWN Reason="undraining"
  update NodeName=node-3 State=RESUME

  update NodeName=node-4 State=DOWN Reason="undraining"
  update NodeName=node-4 State=RESUME

  update NodeName=node-5 State=DOWN Reason="undraining"
  update NodeName=node-5 State=RESUME

  update NodeName=node-6 State=DOWN Reason="undraining"
  update NodeName=node-6 State=RESUME

#+END_SRC


* Sharing a mounted volume via NFS  

  The following script starts a nfs server on the head node and runs code to mount
  the volume on all work nodes. 

  The IP addresses in the =/etc/exports= file belong to the nodes; the IP address
  in the final mount command is the head node's.

  #+BEGIN_SRC shell :exports code :results none
    pdsh -a sudo apt-get -yq install nfs-common
    sudo apt-get install -yq nfs-kernel-server

    sudo -- sh -c 'cat > /etc/exports  << "EOF"
        /data 192.168.1.7(rw,sync,no_subtree_check)
        /data 192.168.1.5(rw,sync,no_subtree_check)
        /data 192.168.1.14(rw,sync,no_subtree_check)
        /data 192.168.1.10(rw,sync,no_subtree_check)
        /data 192.168.1.9(rw,sync,no_subtree_check)

    EOF'
    sudo /etc/init.d/rpcbind restart
    sudo /etc/init.d/nfs-kernel-server restart
    sudo exportfs -r

    pdsh -a sudo mkdir /data
    pdsh -a sudo chown -R ubuntu:ubuntu /data 
    pdsh -a sudo mount 192.168.1.12:/data /data      

    pdsh -a ls /data 

  #+END_SRC

  To stop the NFS server (i.e. before un-mounting the volume): 

  service nfs-kernel-server stop



* Install Docker 

  #+BEGIN_SRC shell :exports code :results none
    pdsh -a sudo apt  install docker.io -yq 
  #+END_SRC

  Add myself to docker group... 
  #+BEGIN_SRC shell
pdsh -a sudo gpasswd -a $USER docker
  #+END_SRC

* Install R (3.5+) on all nodes 

  #+BEGIN_SRC shell :exports code :results none 

    sudo add-apt-repository ppa:marutter/rrutter3.5

    sudo apt install r-api-3.5
    pdsh -a sudo add-apt-repository ppa:marutter/rrutter3.5
    pdsh -a sudo apt update 
    pdsh -a sudo apt install -yq default-jdk r-api-3.5 libxml2-dev libcurl4-openssl-dev openssl libssl-dev  libssh2-1-dev git 
  #+END_SRC

* Install R packages 

  Need to add double quotes for pdsh to work - all other double quotes need to be
  escaped.

  #+BEGIN_SRC sh
    pdsh -a "sudo apt-get install -yq libhdf5-dev"
    pdsh -a "sudo R -e 'install.packages(c(\"Seurat\",\"tidyverse\",\"optparse\",\"reshape2\",\"devtools\",\"reshape\"), repos=\"http://cran.us.r-project.org\", Ncpus = 16)'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"GSVA\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"biomaRt\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"DropletUtils\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"ensembldb\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"EnsDb.Hsapiens.v86\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"EnsDb.Mmusculus.v79\")'"
    pdsh -a "sudo R -e 'devtools::install_github(\"dviraran/SingleR\")'"
    pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(\"scater\")'"
    pdsh -a "sudo R -e 'source(\"http://cf.10xgenomics.com/supp/cell-exp/rkit-install-2.0.0.R\")'"

  #+END_SRC

  #+RESULTS:

 #+BEGIN_SRC sh
   sudo apt-get install -yq libhdf5-dev
   sudo R -e 'install.packages(c("Seurat","tidyverse","optparse","reshape2","devtools","reshape"),dependencies = TRUE, repos="http://cran.us.r-project.org", Ncpus = 16)'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("GSVA")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("biomaRt")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("DropletUtils")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("ensembldb")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("EnsDb.Hsapiens.v86")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("EnsDb.Mmusculus.v79")'
   sudo R -e 'devtools::install_github("dviraran/SingleR")'
   sudo R -e 'source("https://bioconductor.org/biocLite.R");biocLite("scater")'
   sudo R -e 'source("http://cf.10xgenomics.com/supp/cell-exp/rkit-install-2.0.0.R")'

  #+END_SRC

  #+RESULTS:

* Add users


  #+BEGIN_SRC sh 

    sudo addgroup --gid 1005 analysis 
    sudo useradd -m -b  /home -d /home/melvin  -g 1005 -u 1001 melvin
    sudo useradd -m -b  /home -d /home/andre  -g 1005 -u 1002 andre
    pdsh -a sudo addgroup --gid 1005 analysis 
    pdsh -a sudo useradd -m -b  /home -d /home/melvin  -g 1005 -u 1001 melvin
    pdsh -a sudo useradd -m -b  /home -d /home/andre  -g 1005 -u 1002 andre


  #+END_SRC

  Set password

  #+BEGIN_SRC sh 
    sudo passwd andre
    sudo passwd melvin 
  #+END_SRC
 
