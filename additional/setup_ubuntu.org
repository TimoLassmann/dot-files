#+TITLE:  Notes on setting up an Ubuntu 18 server
#+AUTHOR: Timo Lassmann
#+EMAIL:  timo.lassmann@telethonkids.org.au
#+DATE:   2019-01-16
#+LATEX_CLASS: report
#+OPTIONS:  toc:nil
#+OPTIONS: H:4
#+LATEX_CMD: pdflatex
#+PROPERTY: header-args:bash   :eval never-export
#+PROPERTY: header-args:java   :eval never-export

#+BEGIN_SRC emacs-lisp  :results none :exports none 

  (setq org-latex-listings 'minted)
  (setq org-latex-minted-options
        '(("frame" "lines") ("linenos=true")("breaklines")))


#+END_SRC

* Introduction 
  This document describes how to set up a ubuntu 18 server for computational biology work.

  Essentials: 
  #+BEGIN_SRC bash 

    sudo add-apt-repository universe
    sudo apt update
    sudo apt upgrade -y
    sudo apt install -y zsh autoconf libtool make cmake git screen zsh r-base libboost-all-dev nfs-common nfs-kernel-server gdebi-core libpoppler-dev libpoppler-glib-dev libpoppler-private-dev xrdp valgrind exuberant-ctags global
    sudo apt install -y libcurl4-openssl-dev
    sudo apt install -y libxml2-dev
    sudo apt install -y python-pip
    sudo apt install -y python-numpy
    sudo apt install -y rpm
    sudo apt install -y cifs-utils samba smbclient
    sudo apt-get install -y \
         build-essential \
         libssl-dev \
         uuid-dev \
         libgpgme11-dev \
         squashfs-tools \
         libseccomp-dev \
         pkg-config
    sudo apt install -y h5utils hdf5-tools libhdf4-0 libmatheval1
    sudo apt install -y libhdf5-dev hdf5-helpers libaec-dev libhdf5-cpp-100
  #+END_SRC

  Desktop... 

  #+BEGIN_SRC bash 
    sudo apt install ubuntu-desktop

  #+END_SRC
  (maybe a reboot is necessary) 
  R-studio 
  #+BEGIN_SRC bash 
    wget https://download1.rstudio.org/desktop/bionic/amd64/rstudio-1.2.1335-amd64.deb
    sudo gdebi rstudio-1.2.1335-amd64.deb
  #+END_SRC

  Install GO 

  Visit the Go download page and pick a package archive to download. Copy the link address and download with wget. Then extract the archive to /usr/local (or use other instructions on go installation page).
  #+BEGIN_SRC bash
    export VERSION=1.11 OS=linux ARCH=amd64 && \
        wget https://dl.google.com/go/go$VERSION.$OS-$ARCH.tar.gz && \
        sudo tar -C /usr/local -xzvf go$VERSION.$OS-$ARCH.tar.gz && \
        rm go$VERSION.$OS-$ARCH.tar.gz

  #+END_SRC

  Then, set up your environment for Go.
  #+BEGIN_SRC bash
    echo 'export GOPATH=${HOME}/go' >> ~/.bashrc && \
        echo 'export PATH=/usr/local/go/bin:${PATH}:${GOPATH}/bin' >> ~/.bashrc && \
        source ~/.bashrc
  #+END_SRC

  If you are installing Singularity v3.0.0 you will also need to install dep for dependency resolution.

  #+BEGIN_SRC bash
    go get -u github.com/golang/dep/cmd/dep
  #+END_SRC




* Add users 

  #+NAME: users 
  #+BEGIN_SRC bash 
    users=(alexia richard denise genevieve vanessa catherine sarra jenefer melvin)
  #+END_SRC


  #+BEGIN_SRC bash -n :tangle add_user.sh :shebang #!/usr/bin/env bash :noweb yes
    DIR=`pwd`
    USERNAME=
    PASSWORD="password"
    function usage()
    {

        printf "This script will add users to a linux system.\n\n" ;
        printf "The default password is password and needs to be changed by the user.\n\n";
        printf "usage: $0 -n <user name> \n\n" ;
        exit 1;
    }

    while getopts n:  opt
    do

        case ${opt} in
            n) USERNAME=${OPTARG};;
            ,*) usage;;
        esac
    done
    if [ "${USERNAME}" == "" ]; then usage; fi

    echo "Creating user $USERNAME"

    sudo useradd  -b /data/workspace/ -g analysis -m $USERNAME

    echo "$USERNAME:password" | sudo chpasswd


  #+END_SRC

  #+BEGIN_SRC bash -n :tangle add_all_users.sh :shebang #!/usr/bin/env bash :noweb yes
    <<users>>

    echo "Current number of users: ${#users[*]}"

    echo "Users:"
    for item in ${users[*]}
    do
        printf "\t%s\n" $item
        ./add_user.sh -n $item
    done


  #+END_SRC  
  Create group analysis and add users 
  #+BEGIN_SRC bash 
    sudo addgroup analysis
    ./add_all_users.sh
  #+END_SRC

* mounting NFS volumes 

  Create mount points 

  #+BEGIN_SRC bash 

    sudo mkdir -p /data/raw 
    sudo mkdir -p /data/workspace 
    sudo chown -R root:analysis /data
  #+END_SRC

  Mount the shares :
  #+BEGIN_SRC bash
    sudo mount -t nfs -o noatime,vers=3,proto=tcp,rsize=1048576,wsize=1048576,timeo=10000,hard,intr,nolock honas03-tkiprod01.ichr.uwa.edu.au:/HOGRD01_RAWData /data/raw 
    sudo mount -t nfs -o noatime,vers=3,proto=tcp,rsize=1048576,wsize=1048576,timeo=10000,hard,intr,nolock honas03-tkiprod01.ichr.uwa.edu.au:/HOGRD01_WorkSpace /data/workspace
  #+END_SRC

  Unmount:

  #+BEGIN_SRC bash
    sudo umount /data/raw 
    sudo umount /data/workspace 
  #+END_SRC

  To automatically mount disks when booting the machine add these lines to fstab: 

* mounting samba (?) 


#+BEGIN_SRC bash

sudo mount.cifs -o username=XXXXX,workgroup=ichr.uwa.edu.au,domain=ichr.uwa.edu.au,uid=,gid=analysis  //ichr.uwa.edu.au/file\ workspace/Instruments/MiniSeq   dfs-drive
#+END_SRC


* Scratch drive


/dev/sdb on /data/scratch type btrfs (rw,relatime,space_cache,subvolid=5,subvol=/)


* Fix X2GO

  To get rstudio to work with x2go(note that is works via ssh -X <server> rstudio  and the server / browser version)

  edit:

  #+BEGIN_EXAMPLE bash 
  sudo emacs /etc/x2go/x2goagent.options
  #+END_EXAMPLE

  and un-comment this line: 

  X2GO_NXAGENT_DEFAULT_OPTIONS+=" -extension GLX" 




* Install new(er) R version 

  #+BEGIN_SRC bash 
    sudo apt install apt-transport-https software-properties-common
    sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
    sudo add-apt-repository 'deb https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/'
    sudo apt update
    sudo apt install r-base

  #+END_SRC

* Install docker.io 


  #+BEGIN_SRC bash

    sudo apt  install docker.io


  #+END_SRC


  Create daemon file: 

  sudo emacs /etc/docker/daemon.json
  #+BEGIN_SRC bash 

    {
        "log-level":        "error",
        "graph": "/data/scratch/docker",
        "storage-driver":   "overlay2"
    }

  #+END_SRC

  To stop docker: 
#+BEGIN_SRC bash 

sudo service docker stop

#+END_SRC
* Slurm 

  #+BEGIN_SRC bash 

    sudo apt install slurmctld slurmd 

  #+END_SRC

  This will install the slurm daemon, server munge stuff and create a slurm user. Also the following directories are creates: 

  =/var/run/slurm-llnl= 
  =/var/spool/slurmd= 
  and
  =/etc/slurm-llnl/= 

  Last time I tired this the ownership permissions on =/var/spool/slurmd/= were set incorrectly. To fix this: 

  #+BEGIN_SRC bash 
    sudo chown slurm:slurm /var/spool/slurmd
  #+END_SRC

  Slurm requires two configuration files to be places in the =/etc/slurm-llnl/= directory: 

  1) cgroup.conf

  #+BEGIN_SRC text 
    CgroupAutomount=yes
    CgroupReleaseAgentDir="/etc/slurm-llnl/cgroup" 

    ConstrainCores=yes 
    ConstrainDevices=yes
    ConstrainRAMSpace=yes

  #+END_SRC
  
  2) the slurm configuration files. 

  Unfortunately the html configuration tool does not work with the ubuntu slurm version.  The following modifications are necessary: 

  #+BEGIN_SRC text 
    ClusterName=compute-cluster
    ControlMachine=hogrd01
  #+END_SRC

  and make sure the parameters below point to the directories above: 

  #+BEGIN_SRC text 
    SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
    SlurmctldPort=6817
    SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
    SlurmdPort=6818
    SlurmdSpoolDir=/var/spool/slurmd

    StateSaveLocation=/var/spool/slurmd

  #+END_SRC
  AND 

  #+BEGIN_SRC text
    # SCHEDULING
    #DefMemPerCPU=0
    FastSchedule=1
    #MaxMemPerCPU=0
    #SchedulerTimeSlice=30
    SchedulerType=sched/backfill
    SelectType=select/cons_res
    SelectTypeParameters=CR_Core_Memory

  #+END_SRC
  The last  line in necessary 4 memory based scheduling to work! 

  Here is a more recent complete slurm.conf after the server was updated 
  #+BEGIN_SRC text 
    # slurm.conf file generated by configurator.html.
    # Put this file on all nodes of your cluster.
    # See the slurm.conf man page for more information.
    #

    ClusterName=compute-cluster
    ControlMachine=hogrd01

    #SlurmctldHost=
    #
    #DisableRootJobs=NO
    #EnforcePartLimits=NO
    #Epilog=
    #EpilogSlurmctld=
    #FirstJobId=1
    #MaxJobId=999999
    #GresTypes=
    #GroupUpdateForce=0
    #GroupUpdateTime=600
    #JobFileAppend=0
    #JobRequeue=1
    #JobSubmitPlugins=1
    #KillOnBadExit=0
    #LaunchType=launch/slurm
    #Licenses=foo*4,bar
    #MailProg=/bin/mail
    #MaxJobCount=5000
    #MaxStepCount=40000
    #MaxTasksPerNode=128
    MpiDefault=none
    #MpiParams=ports=#-#
    #PluginDir=
    #PlugStackConfig=
    #PrivateData=jobs
    ProctrackType=proctrack/cgroup
    #Prolog=
    #PrologFlags=
    #PrologSlurmctld=
    #PropagatePrioProcess=0
    #PropagateResourceLimits=
    #PropagateResourceLimitsExcept=
    #RebootProgram=
    ReturnToService=1
    #SallocDefaultCommand=
    SlurmctldPidFile=/var/run/slurm-llnl/slurmctld.pid
    SlurmctldPort=6817
    SlurmdPidFile=/var/run/slurm-llnl/slurmd.pid
    SlurmdPort=6818
    SlurmdSpoolDir=/var/spool/slurmd
    SlurmUser=slurm
    #SlurmdUser=root
    #SrunEpilog=
    #SrunProlog=
    StateSaveLocation=/var/spool/slurmd
    SwitchType=switch/none
    #TaskEpilog=
    TaskPlugin=task/affinity
    TaskPluginParam=Sched
    #TaskProlog=
    #TopologyPlugin=topology/tree
    #TmpFS=/tmp
    #TrackWCKey=no
    #TreeWidth=
    #UnkillableStepProgram=
    #UsePAM=0
    #
    #
    # TIMERS
    #BatchStartTimeout=10
    #CompleteWait=0
    #EpilogMsgTime=2000
    #GetEnvTimeout=2
    #HealthCheckInterval=0
    #HealthCheckProgram=
    InactiveLimit=0
    KillWait=30
    #MessageTimeout=10
    #ResvOverRun=0
    MinJobAge=300
    #OverTimeLimit=0
    SlurmctldTimeout=120
    SlurmdTimeout=300
    #UnkillableStepTimeout=60
    #VSizeFactor=0
    Waittime=0
    #
    #
    # SCHEDULING
    DefMemPerCPU=8000
    FastSchedule=1
    MaxMemPerCPU=128000
    #SchedulerTimeSlice=30
    SchedulerType=sched/backfill
    SelectType=select/cons_res
    SelectTypeParameters=CR_CPU_Memory
    #
    #
    # JOB PRIORITY
    #PriorityFlags=
    #PriorityType=priority/basic
    #PriorityDecayHalfLife=
    #PriorityCalcPeriod=
    #PriorityFavorSmall=
    #PriorityMaxAge=
    #PriorityUsageResetPeriod=
    #PriorityWeightAge=
    #PriorityWeightFairshare=
    #PriorityWeightJobSize=
    #PriorityWeightPartition=
    #PriorityWeightQOS=
    #
    #
    # LOGGING AND ACCOUNTING
    #AccountingStorageEnforce=0
    #AccountingStorageHost=
    #AccountingStorageLoc=
    #AccountingStoragePass=
    #AccountingStoragePort=
    AccountingStorageType=accounting_storage/none
    #AccountingStorageUser=
    AccountingStoreJobComment=YES

    #DebugFlags=
    #JobCompHost=
    #JobCompLoc=
    #JobCompPass=
    #JobCompPort=
    JobCompType=jobcomp/none
    #JobCompUser=
    #JobContainerType=job_container/none
    JobAcctGatherFrequency=30
    JobAcctGatherType=jobacct_gather/none
    SlurmctldDebug=info
    #SlurmctldLogFile=
    SlurmdDebug=info
    #SlurmdLogFile=
    #SlurmSchedLogFile=
    #SlurmSchedLogLevel=
    #
    #
    # POWER SAVE SUPPORT FOR IDLE NODES (optional)
    #SuspendProgram=
    #ResumeProgram=
    #SuspendTimeout=
    #ResumeTimeout=
    #ResumeRate=
    #SuspendExcNodes=
    #SuspendExcParts=
    #SuspendRate=
    #SuspendTime=
    #
    #
    # COMPUTE NODES
    NodeName=hogrd01 CPUs=64 RealMemory=400000 Sockets=2 CoresPerSocket=32 ThreadsPerCore=1
    PartitionName=prod Nodes=hogrd01 Default=YES MaxTime=INFINITE  MaxCPUsPerNode=64 State=UP

  #+END_SRC

  Then start daemon / server: 

  #+BEGIN_SRC bash 

    sudo systemctl enable slurmd
    sudo systemctl enable slurmctld 

    sudo systemctl start slurmd
    sudo systemctl start slurmctld 
  #+END_SRC

  Undrain nodes:
  #+BEGIN_SRC bash
    sudo scontrol 
    update NodeName=hogrd01 State=DOWN Reason="undraining"
    update NodeName=hogrd01 State=RESUME
    quit
  #+END_SRC

  To shut down: 

  #+BEGIN_SRC bash 
    sudo systemctl stop slurmd
    sudo systemctl stop slurmctld 


  #+END_SRC
  
* Singularity


  #+BEGIN_SRC bash

    export VERSION=3.2.1 && # adjust this as necessary \

        wget https://github.com/sylabs/singularity/releases/download/v${VERSION}/singularity-${VERSION}.tar.gz && \
            tar -xzf singularity-${VERSION}.tar.gz && \
            cd singularity


  #+END_SRC

  Install

  #+BEGIN_SRC bash

    ./mconfig && \
        make -C builddir && \
        sudo make -C builddir install



  #+END_SRC

* udocker 

  cromwell + udocker + slurm works

  Install udocker: 

  #+BEGIN_SRC bash

    curl https://raw.githubusercontent.com/indigo-dc/udocker/master/udocker.py > udocker
    chmod u+rx ./udocker
    ./udocker install

  #+END_SRC

  place =udocker=  somewhere in the path. 


  IMPORTANT: by default udocker will store images in the user's home directory =~/.udocker/=. To use a different location:

  #+BEGIN_SRC bash
    export UDOCKER_DIR="/data/scratch/udocker"
  #+END_SRC

  Execute cromwell jobs like this: 

  #+BEGIN_SRC bash    
    java -jar -Dconfig.file=/home/timo/testslurm/testing_crom.conf  -Dbackend.default=slurm -Ddocker.hash-lookup.enabled=false ~/program/cromwell-44.jar run Optimus.wdl  -i m1_run.json
  #+END_SRC


  Workaround (?) for being unable to pull images from google cloud:
  #+BEGIN_SRC bash
    docker save centos:7 > centos
    udocker load -i centos

  #+END_SRC




* example cromwell config file

  #+BEGIN_SRC java
    include required(classpath("application"))


        docker {
        hash-lookup {
            enable = false
                }
    }

    backend {
        default = "slurm"
            providers {
            slurm {
                actor-factory = "cromwell.backend.impl.sfs.config.ConfigBackendLifecycleActorFactory"
                config {
                    script-epilogue = "sleep 30"
                    concurrent-job-limit = 10
                    run-in-background = true
                    runtime-attributes = """
              Int cpu = 1
              Int? gpu
              Int? time
              Int? memory_mb
              String? slurm_partition
              String? slurm_account
              String? slurm_extra_param
              String? docker        
              """


                    submit = """
              sbatch \
              --export=ALL \
              --wait \        
              -J ${job_name} \
              -D ${cwd} \
              -o ${out} \
              -e ${err} \
              ${"-t " + time*60} \
              -n 1 \
              --ntasks-per-node=1 \
              ${true="--cpus-per-task=" false="" defined(cpu)}${cpu} \
              ${true="--mem=" false="" defined(memory_mb)}${memory_mb}\
              ${"-p " + slurm_partition} \
              ${"--account " + slurm_account} \
              ${"--gres gpu:" + gpu} \
              ${slurm_extra_param} \
              --wrap "/bin/bash ${script}"
              """

                    submit-docker = """
              # Pull the image using the head node, in case our workers don't have network access
              udocker pull ${docker}
              sbatch \
                 --export=ALL \
                 --wait \
                 -J ${job_name} \
                 -D ${cwd} \
                 -o ${out} \
                 -e ${err} \
                 ${"-t " + time*60} \
                 -n 1 \
                 --ntasks-per-node=1 \
                 ${true="--cpus-per-task=" false="" defined(cpu)}${cpu} \
                 ${true="--mem=" false="" defined(memory_mb)}${memory_mb}\
                 ${"-p " + slurm_partition} \
                 ${"--account " + slurm_account} \
                 ${"--gres gpu:" + gpu} \
                 ${slurm_extra_param} \
                 --wrap "udocker run -v ${cwd}:${docker_cwd} ${docker} ${job_shell} ${docker_script}"
              """



                    kill = "scancel ${job_id}"
                    check-alive = "squeue -j ${job_id}"
                    job-id-regex = "Submitted batch job (\\d+).*"
                }
            }

        }
    }




    services {
        LoadController {
            class = "cromwell.services.loadcontroller.impl.LoadControllerServiceActor"
                config {
                # disable it (for login nodes on Stanford SCG, Sherlock)
                control-frequency = 21474834 seconds
            }
        }
    }



    system {
        abort-jobs-on-terminate = true
            graceful-server-shutdown = true
            }

    call-caching {
        enabled = false
            invalidate-bad-cache-results = true
            }


  #+END_SRC
    














