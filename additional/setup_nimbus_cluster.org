#+TITLE:  Setting up a Nimbus slurm cluster
#+AUTHOR: Timo Lassmann
#+EMAIL:  timo.lassmann@telethonkids.org.au
#+DATE:   2018-05-24
#+LATEX_CLASS: report
#+OPTIONS:  toc:nil
#+OPTIONS: H:4
#+LATEX_CMD: xelatex

* Introduction

This tutorial walks you through the steps required to set up a linux cluster on
pawsey's openstack nimbus cloud.


* Setting up and starting nodes 

We highly recommend allocating a minimal amount of storage to each node and use
a single large NFS mounted volume to share data within the cluster. 

1) Follow the instructions here:
https://support.pawsey.org.au/documentation/display/US/Nimbus+-+Launching+an+Instance

NOTE: some of the downstream steps will be easier if you create 5 nodes at once.
This can be accomplished by selecting a name like 'node' and requesting 5 or
more instances. OpenStack will append '-1', '-2' etc to the names. 

2) copy ssh key to main node connected to the internet (so we can connect to
individual nodes..)

3) On the main node add a ssh config file 

Here is an example of how this may look: 

#+BEGIN_SRC shell :tangle ssh_config_nimbus  :exports code :results none
  StrictHostKeyChecking no
  Host single-cell 
  User ubuntu
  Hostname 192.168.1.6
  IdentityFile ~/.ssh/pawsey.key
  Host node-5
  Hostname 192.168.1.16
  User ubuntu
  IdentityFile ~/.ssh/pawsey.key
  Host node-4
  Hostname 192.168.1.7
  User ubuntu
  IdentityFile ~/.ssh/pawsey.key
  Host node-3
  Hostname 192.168.1.15
  User ubuntu
  IdentityFile ~/.ssh/pawsey.key
  Host node-2
  Hostname 192.168.1.14
  User ubuntu
  IdentityFile ~/.ssh/pawsey.key
  Host node-1
  Hostname 192.168.1.12
  User ubuntu
  IdentityFile ~/.ssh/pawsey.key
  
#+END_SRC

Then copy to content to ~/.ssh/config 


4) Install pdsh as describes here

https://support.pawsey.org.au/documentation/display/US/Nimbus+-+Managing+a+VM+Cluster

Especially add nodes in the /etc/genders file:  

sudo vi /etc/genders

#+BEGIN_SRC shell :tangle genders  :exports code :results none
node-1
node-2
node-3
node-4
node-5

#+END_SRC


* Update nodes

  #+BEGIN_SRC shell
    pdsh -a sudo apt-get update 
    pssd -a sudo apt-get -yq upgrade 
  #+END_SRC

* Setup MUNGE & SLURM 
** MUNGE 

*** Install 
   #+BEGIN_SRC shell :tangle basic_node_setup.sh :shebang #!/bin/bash :exports code :results none
 
     pdsh -a sudo apt-get -yq install libmunge-dev libmunge2 munge 
     sudo apt-get -yq install libmunge-dev libmunge2 munge 

   #+END_SRC

*** Synchronise munge keys.

First set up a script to copy the munge key from the home directory into the
right location on all node.

   #+BEGIN_SRC shell :tangle munge_per_node.sh :shebang #!/bin/bash :exports code :results none
     sudo systemctl stop munge
      cd ~
     sudo chown munge: munge.key
     sudo mv munge.key /etc/munge/munge.key

     sudo chmod 400 /etc/munge/munge.key
     sudo systemctl enable munge
     sudo systemctl start munge
   #+END_SRC

Then: 
1) generate the munge key on the head node
2) copy the key to all nodes 
3) run the script above 
4) also copy the key to the correct directory on the head node 
 
   #+BEGIN_SRC shell :tangle generate_sync_munge_key.sh :shebang #!/bin/bash :exports code :results none
     dd if=/dev/random bs=1 count=1024 > munge.key
     pdcp -a munge.key ~/munge.key
     pdcp -a munge_per_node.sh  ~/munge_per_node.sh 
     pdsh -a ./munge_per_node.sh 

     sudo systemctl stop munge
     sudo chown munge: munge.key
     sudo mv munge.key /etc/munge/munge.key

     sudo chmod 400 /etc/munge/munge.key

     sudo systemctl enable munge
     sudo systemctl start munge
     
   #+END_SRC

* Slurm 

Setup for a basic slurm scheduler. The setup below is minimal but works.  

** Generate a slurm config file 

Generate a slurm config file using this web-form: 

https://slurm.schedmd.com/configurator.html

Note: 
1) It is critical to correctly name the head node and the worker nodes. These
   names have to match exactly what is shown in the openstack online management
   console (https://nimbus.pawsey.org.au).

2) Make sure the number of CPUs, main memory etc are set correctly for each
   worker node. If you are unsure how to set these install the slurm daemon on a
   node: 
   ssh nodeXXX 
   sudo apt-get install -yq slurmd 
   and run: 
   slurmd -C 

Finally copy the output of the configurator webpage into a file called
=slurm.conf= in the home directory of your head node. 


** Script to set up node 

   The script below is used to configure each node. This includes moving the
   slurm.conf file created above into the right directory. Note that the IP
   addresses have to be manually edited in this script to match your setup.

   #+BEGIN_SRC shell :tangle setup_host_for_slurm.sh :shebang #!/bin/bash   :exports code :results none
     sudo -- sh -c 'cat > /etc/slurm-llnl/cgroup.conf  << "EOF"
     CgroupAutomount=yes
     CgroupReleaseAgentDir="/etc/slurm-llnl/cgroup" 

     ConstrainCores=yes 
     ConstrainDevices=yes
     ConstrainRAMSpace=yes

     EOF'

     sudo -- sh -c 'cat > /etc/hosts  << "EOF"

     127.0.0.1 localhost
     192.168.1.6 single-cell
     192.168.1.16 node-5
     192.168.1.7 node-4
     192.168.1.15 node-3
     192.168.1.14 node-2
     192.168.1.12 node-1
     # The following lines are desirable for IPv6 capable hosts
     ::1 ip6-localhost ip6-loopback
     fe00::0 ip6-localnet
     ff00::0 ip6-mcastprefix
     ff02::1 ip6-allnodes
     ff02::2 ip6-allrouters
     ff02::3 ip6-allhosts

     EOF'

     sudo mv ~/slurm.conf /etc/slurm-llnl/slurm.conf
     sudo chown slurm: /etc/slurm-llnl/slurm.conf


   #+END_SRC

The following script configures the head node, copies the above script to all
nodes and configures them.

NOTE:

1) Edit the IP address to match your configuration.
2) this script expects a =slurm.conf= file in the home directory of user ubuntu
on the head node.

  #+BEGIN_SRC shell :tangle setup_hosts_file.sh :shebang #!/bin/bash :exports code :results none
sudo apt-get  install slurmctld slurmctld-dbg slurmdbd slurmdbd-dbg
pdsh -a sudo apt-get update 
pdsh -a sudo apt-get -yq upgrade 
pdsh -a sudo apt-get -yq install slurmd pdsh
pdcp -a cp slurm.conf ~/slurm.conf
pdcp -a setup_host_for_slurm.sh  ~/setup_host_for_slurm.sh
pdsh -a ./setup_host_for_slurm.sh 
pdsh -a grep single /etc/hosts 

sudo mv ~/slurm.conf /etc/slurm-llnl/slurm.conf
sudo chown slurm: /etc/slurm-llnl/slurm.conf
sudo -- sh -c 'cat > /etc/hosts  << "EOF"

127.0.0.1 localhost
192.168.1.6 single-cell
192.168.1.16 node-5
192.168.1.7 node-4
192.168.1.15 node-3
192.168.1.14 node-2
192.168.1.12 node-1
# The following lines are desirable for IPv6 capable hosts
::1 ip6-localhost ip6-loopback
fe00::0 ip6-localnet
ff00::0 ip6-mcastprefix
ff02::1 ip6-allnodes
ff02::2 ip6-allrouters
ff02::3 ip6-allhosts

EOF'


#+END_SRC

** Start Slurm 


 #+BEGIN_SRC shell :exports code :results none
sudo systemctl stop slurmctld
sudo systemctl start  slurmctld

pdsh -a sudo systemctl stop slurmd
pdsh -a sudo systemctl enable slurmd
pdsh -a sudo slurmd

#+END_SRC



After this you should be able to see all nodes using the =sinfo= command. 


* Sharing a mounted volume via NFS  

  The following script starts a nfs server on the head node and runs code to mount
  the volume on all work nodes. 

  The IP addresses in the =/etc/exports= file belong to the nodes; the IP address
  in the final mount command is the head node's.

  #+BEGIN_SRC shell :exports code :results none
    pdsh -a sudo apt-get -yq install nfs-common
    sudo apt-get install -yq nfs-kernel-server

    sudo -- sh -c 'cat > /etc/exports  << "EOF"
    /data 192.168.1.16(rw,sync,no_subtree_check)
    /data 192.168.1.7(rw,sync,no_subtree_check)
    /data 192.168.1.15(rw,sync,no_subtree_check)
    /data 192.168.1.14(rw,sync,no_subtree_check)
    /data 192.168.1.12(rw,sync,no_subtree_check)

    EOF'
    sudo /etc/init.d/rpcbind restart
    sudo /etc/init.d/nfs-kernel-server restart
    sudo exportfs -r

    pdsh -a sudo mkdir /data
    pdsh -a sudo chown -R ubuntu:ubuntu /data 
    pdsh -a sudo mount 192.168.1.6:/data /data      

    pdsh -a ls /data 

  #+END_SRC

To stop the NFS server (i.e. before un-mounting the volume): 

service nfs-kernel-server stop

* Let's test cellranger 

 #+BEGIN_SRC shell :tangle run_cellranger_test.sh :shebang #!/bin/bash :exports code :results none
#SBATCH --time=01:00:00
#SBATCH --nodes=1 
#SBATCH --ntasks-per-node=16

export PATH=/data/programs/cellranger-2.1.1:$PATH

cd /data
mkdir $(hostname) 
cd $(hostname) 
cellranger testrun --id=tiny
  #+END_SRC


* A note on extending btrfs filesystems: 

1) umount
2) detach in openstack console 
3) resize 
4) re-attach 
5)  mount /dev/vdc /data
6) resize using: 
btrfs filesystem resize max /data/

* Install bcl2fastq2 on all nodes: 

  #+BEGIN_SRC shell
    pdsh -a sudo apt-get install -yq alien
    pdcp -a bcl2fastq2-v2.20.0.422-Linux-x86_64.rpm ~/bcl2fastq2-v2.20.0.422-Linux-x86_64.rpm 

    pdsh -a sudo alien -i bcl2fastq2-v2.20.0.422-Linux-x86_64.rpm
  #+END_SRC



* (old stuff - ignore from here) Upgrade software 




** slurm via apt-get packages... 

Slurm libraries
apt-get install slurm-llnl
sudo apt-get install -yq  mailutils



** install R packages  


   ubuntu 18 should come with R version 3.4 ...

   #+BEGIN_SRC shell :tangle basic_R_setup.sh :shebang #!/bin/bash :exports code :results none
     pdsh -a "sudo R -e 'install.packages(c(\"devtools\",\"curl\"),repos=\"http://cran.us.r-project.org\")' "
     pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite()' "
     pdsh -a "sudo R -e 'install.packages(c(\"Seurat\",\"tidyverse\",\"optparse\",\"reshape\"),repos=\"http://cran.us.r-project.org\")'"
     pdsh -a "sudo R -e 'source(\"https://bioconductor.org/biocLite.R\");biocLite(c(\"biomaRt\"),repos=\"http://cran.us.r-project.org\")'"
   #+END_SRC


* Scratch 


Need to manually create and attach scratch[1-5] volumes... 

NOPE - simply allocate 500GB to reach instance... 
 
* Cellranger  

** Move reference data to all nodes: 

  #+BEGIN_EXAMPLE shell
  cd /data/reference 
  pdcp -a refdata-cellranger-hg19-and-mm10-2.1.0.tar.gz  ~/refdata-cellranger-hg19-and-mm10-2.1.0.tar.gz
  pdsh -a tar -zxvf refdata-cellranger-hg19-and-mm10-2.1.0.tar.gz
  pdsh -a rm refdata-cellranger-hg19-and-mm10-2.1.0.tar.gz
  #+END_EXAMPLE

** Move cellranger software to all nodes: 
 #+BEGIN_EXAMPLE shell
  cd /data/programs 
  pdcp -a cellranger-2.1.1.tar.gz  ~/cellranger-2.1.1.tar.gz
  pdsh -a tar -zxvf cellranger-2.1.1.tar.gz
  pdsh -a rm cellranger-2.1.1.tar.gz
  #+END_EXAMPLE
 
** Test installation :

   #+BEGIN_EXAMPLE shell

   pdsh -a "export PATH=~/cellranger-2.1.1:$PATH; cellranger testrun --id=tiny"
     #+END_EXAMPLE

* Install PBS 


** enable connection between work and main node 

   #+BEGIN_SRC shell :tangle node_ssh_config.txt :exports code :results none
     StrictHostKeyChecking no
     UserKnownHostsFile=/dev/null
     Host single-cell
     Hostname 192.168.1.6
     User ubuntu
     IdentityFile ~/.ssh/pawsey.key
   #+END_SRC

#+BEGIN_SRC shell :tangle munge_slurm_setup.sh :shebang #!/bin/bash :exports code :results none
pdcp -a ~/.ssh/pawsey.key ~/.ssh/pawsey.key
pdcp -a ~/node_ssh_config.txt  ~/.ssh/config
   #+END_SRC

* Play with slurm 

** Install munge
#+BEGIN_SRC shell :tangle
sudo apt-get install libmunge-dev libmunge2 munge
sudo systemctl enable munge
sudo systemctl start munge

#+END_SRC

** install mariadb 

#+BEGIN_SRC shell 
  sudo apt-get install mariadb-server
  sudo systemctl enable mysql
  sudo systemctl start mysql
#+END_SRC

** Download, build, and install Slurm

pdsh -a sudo apt install slurm-llnl

sudo apt install  slurmdbd slurmctld slurm-client


wget https://download.schedmd.com/slurm/slurm-17.11.6.tar.bz2
tar xvjf slurm-17.11.6.tar.bz2
cd slurm-17.11.6
 ./configure --prefix=/tmp/slurm-build --sysconfdir=/etc/slurm --enable-pam --with-pam_dir=/lib/x86_64-linux-gnu/security/
 make
 make contrib
 sudo make install
 cd ..
 fpm -s dir -t deb -v 1.0 -n slurm-17.11.6 --prefix=/usr -C /tmp/slurm-build .
dpkg -i slurm-17.11.6_1.0_amd64.deb
sudo useradd slurm 
sudo mkdir -p /etc/slurm /etc/slurm/prolog.d /etc/slurm/epilog.d /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm
sudo chown slurm /var/spool/slurm/ctld /var/spool/slurm/d /var/log/slurm


sudp cp ~/slurm.conf /etc/slurm/
sudo cp slurmdbd.service /etc/systemd/system/
sudo cp slurmctld.service /etc/systemd/system/

sudo systemctl stop slurmdbd
sudo systemctl stop slurmctld
sudo systemctl daemon-reload
sudo systemctl enable slurmdbd
sudo systemctl start slurmdbd
sudo systemctl enable slurmctld
sudo systemctl start slurmctld

sudo systemctl daemon-reload
sudo systemctl disable slurmdbd
sudo systemctl stop slurmdbd
sudo systemctl disable slurmctld
sudo systemctl stop slurmctld






** Install munge / mariadb 

   #+BEGIN_SRC shell :tangle munge_slurm_setup.sh :shebang #!/bin/bash :exports code :results none
     pdsh -a sudo apt-get update 
     pdsh -a sudo apt-get -yq upgrade 
     pdsh -a sudo apt-get -yq install mariadb-server munge rng-tools 
     pdsh -a sudo apt-get -yq upgrade 
     pdsh -a sudo apt-get -yq autoremove 

     sudo apt-get -yq install mariadb-server munge rng-tools 

     sudo rngd -r /dev/urandom
     sudo su 
     dd if=/dev/random bs=1 count=1024 >/etc/munge/munge.key

     chown munge: /etc/munge/munge.key
     chmod 400 /etc/munge/munge.key
exit 
     
     sudo cp /etc/munge/munge.key . 
     sudo chown ubuntu: munge.key

     pdcp -a munge.key  ~/munge.key
     pdsh -a sudo chown munge: ~/munge.key
     pdsh -a sudo chmod 400  ~/munge.key
     pdsh -a sudo cp  ~/munge.key  /etc/munge/munge.key

    
     pdsh -a sudo /etc/init.d/munge start 
     sudo /etc/init.d/munge start 
   #+END_SRC


pdcp cellranger to all nodes 

unpack 

compile... 





